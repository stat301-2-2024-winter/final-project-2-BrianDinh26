---
title: "Final Report on Price Prediction in Online Used Car Sales"
subtitle: |
  | Class: STAT 301-2, Winter 2024, Northwestern University
author: "Brian Dinh"
date: today
fig-cap-location: top
format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: true
  warning: false
number-sections: true
from: markdown+emoji 
---

::: {.callout-tip icon=false}

## Github Repository Link

[This is the link to the GitHub Repository.](https://github.com/stat301-2-2024-winter/final-project-2-BrianDinh26.git)

:::
```{r}
#| echo: false
#| label: library-load

# note to self, use knitr::kable, not DT::datatable for captions.

# turn off scientific notation
options(scipen=999)

# load in all figures used for this analysis.
library(tidyverse)
library(here)

load(here("figures/data_exploration.rda"))
load(here("figures/kitchen_sink_metric_table.rda"))
load(here("figures/best_hyperparameters.rda"))
load(here("figures/table_null.rda"))
load(here("figures/engineered_final_table.rda"))
load(here("figures/assess_final.rda"))
```



# Introduction {#sec-introduction}
For this project, I used a dataset^[Kirill Lepchenkov's Used Cars Dataset ([see website](https://www.kaggle.com/datasets/lepchenkov/usedcarscatalog/data))] on online used car sales, which described the characteristics of various car listings on the website in factor, logical, and numerical form.

In this predictive model building project, I was motivated to learn more about what variables affect the price of a car being resold and what models were the best for creating price predictions. This sort of model would be useful in figuring out if a person is overvaluing or undervaluing their car price listing relative to the market, or if a buyer is getting a good deal or not online.

# Data Overview {#sec-data-overview}
For the used cars dataset, there are 30 variables and 38,531 observations. There are 10 factor variables, 7 numerical variables, and 13 logical variables. There are 38,531 observations. I will use the variable `price_usd` as my response variable.

```{r}
#| echo: false
#| label: fig-og-log-price
#| fig-cap: "Comparison of Distribution of Original Price and Log Transformed Price in Whole Dataset"

og_v_log_price
```
Based on @fig-og-log-price, the original distribution of `price_usd` is extremely skewed. In comparison, the log transformed version of `price_usd`, although skewed, is less disparate, and therefore I will use the log transformed version of `price_usd` in my model building.

```{r}
#| echo: false
#| label: fig-missing-check
#| fig-cap: "View of Missingness in Whole Dataset by Variable"

missingness_check
```

According to @fig-missing-check, there are only misingness issues with the `engine_capacity` variable, with 10 observations out of the 38,531 observations registering as NA. To deal with this missingness, I will impute the missing observations with the k-nearest neighbors method in the recipes for the model.

Additionally, I used an 80/20 split of my training data to explore possible relationships in the training dataset, meaning that 80% of the selected data from the training dataset will be used to explore relationships. Based on my exploratory data analysis, there appear to be correlations between `duration_listed` and `up_counter`. For more information on the exploration methods, please refer to  

Next, I explored the distributions of the numerical variables in my training dataset. Based on my findings, it appears that the variables `engine_capacity` and `number_of_photos` had highly skewed distributions. Therefore, I believe that a log transformation would be appropriate for these variables. According to @fig-original-vs-log-1 and @fig-original-vs-log-2, the log transformations for these variables help make it more balanced out, allowing for a potentially more effective model.

```{r}
#| echo: false
#| fig-cap: "Exploration of Categorical Variables"
#| label: fig-categorical-exploration-1

categorical_exploration_1
```
```{r}
#| echo: false
#| fig-cap: "Car Year Produced Compared to Manufacturer"
#| label: fig-categorical-exploration-2

manf
```

Last of all, I explored how categorical variables affected the distributions of numeric variables in the dataset. Based on my exploration, as seen in @fig-categorical-exploration-1 and @fig-categorical-exploration-2, there were associations between `engine_type` and `engine_capacity`, `is_exchangeable` and `duration_listed`, and `manufacturer_name` and `year_produced`. 


# Methods {#sec-methods}
For the used cars dataset, I will implement an 80 / 20 split, meaning approximately 80% of the data will be used for training the model, and 20% of the data will be used to test how successful the model is. 

In the models I am building, I am using the Root Mean Squared Error (RMSE) as my primary metric to evaluate because I am using regression to predict the price of a car online. The RMSE is the average difference between values predicted by a model and the actual values, meaning the lower the better. The model with the lowest RMSE relative to the other models will be selected as the final model.

I will be using a total of six models for this report. First off, my baseline model will be the null model, which will serve as a measuring stick for the performance of more complex models. No main arguments will be provided to the null model.

I will be building two linear models that will be using a recipe for parametric models: A standard linear regression model and an elastic net model. The standard linear regression model does not have any parameters that will be tuned. For the elastic net model, I will tune the penalty and mixture of the model.

Additionally, I will be building three models that will be using a tree-based recipe: A random forest model, a boosted tree model, and a k-nearest neighbors model. The random forest model will tune the minimum number of datapoints for a split in a node and the number of predictors that will be randomly sampled at each split, while the number of trees will bet set at 100. For the boosted tree model, I will tune the minimum number of datapoints for a split in a node, the number of predictors that will be randomly sampled at each split, and the rate at which the boosting algorithm adapts from iteration-to-iteration. For the k-nearest neighbors model, I will tune the number of neighbors in the model, which are the number of close datapoints used for predicting.

The resampling technique used will be v-fold cross-validation, with 10 folds and 5 repeats, meaning there will be 10 sets of performance metrics and 5 repetitions of the process. This process is meant to improve the model's accuracy and prevent overfitting through its repetitions and averaging out the results.

## Recipes 
The first recipe that will be used is a baseline, "kitchen sink" recipe, which is a basic recipe with minimal steps that will allow for the model to run. This recipe will be used for the null model, which will act as my baseline.

The second recipe used will be a feature engineered recipe for linear models, where the recipe is modified to include possible interactions and transformations that appear to be correlated or related. In particular, for the case of my used cars dataset, my feature engineered recipe includes performing a log transformation on the `engine_capacity` and `number_of_photos` variables and creating interactions between `duration_listed` and `up_counter`, `engine_type` and `engine_capacity`, `is_exchangeable` and `duration_listed`, and `manufacturer_name` and `year_produced`. Additionally, I removed `color` and `engine_has_gas` in this recipe because `color` had too many variables to consider that did not appear to modify price much and `engine_has_gas` followed similarly to `engine_type`. These decisions were made based on the data exploration performed in @sec-data-exploration, where I concluded that there were interactions between these variables or a need to transform the data in order to improve upon the model.

The third recipe used will be a feature engineered recipe for tree-based models. The recipe is almost the same as the second recipe, but all factor variables will instead be one-hot encoded because the recipe will be used on tree-based models.

# Model Building & Selection Results
## Baseline Null Model with Kitchen Sink Recipe Results
Initially, I applied the baseline, "kitchen sink" recipe to the null model and found its RMSE, as RMSE is the metric by which I am comparing models and will determine the final model. 

```{r}
#| echo: false
#| tbl-cap: "RMSEs of Baseline Recipe Trained Modelsl"
#| label: tbl-rmse-baseline 

kitchen_sink_metric_table |> 
  knitr::kable()
```

According to @tbl-rmse-baseline, the best performing model was the random forest model, with an RMSE of approximately 

## Best Hyperparameters for Feature Engineered Recipe Models
Next, I used the feature engineered recipes for the parametric and tree-based models and found the best hyperparameters for the models that were tuned, based on the lowest RMSE found for each model. For the null model and the ordinary linear regression model, there was no tuning of hyperparameters involved because they do not have relevant hyperparameters to look at.

```{r}
#| echo: false
#| tbl-cap: "Best Hyperparameters for the Elastic Net Model"
#| label: tbl-elastic-hyper

elastic_select |> 
  knitr::kable(digits = c(15, 1, NA))
```
According to @tbl-elastic-hyper, the best hyperparameters for the elastic net model is a penalty value of 0.0000000001, which is effectively 0, and a mixture of 0.5, indicating a mix between the lasso and ridge regression.

```{r}
#| echo: false
#| tbl-cap: "Best Hyperparameters for the K-Nearest Neighbors Model"
#| label: tbl-knn-hyper

knn_select |> 
  knitr::kable()
```
Based on @tbl-knn-hyper, the best model for the k-nearest neighbors model has tuned the value of neighbors to 15. 

```{r}
#| echo: false
#| tbl-cap: "Best Hyperparameters for the Boosted Tree Model"
#| label: tbl-bt-hyper

bt_select |> 
  knitr::kable()
```
According to @tbl-bt-hyper, the best hyperparameters for the boosted tree model are 14 for the number of randomly drawn candidate variables, 2 for the minimum number of models, and a learn rate of approximately 0.6309573.

```{r}
#| echo: false
#| tbl-cap: "Best Hyperparameters for the Random Forest Model"
#| label: tbl-rf-hyper

rf_select |> 
  knitr::kable()
```
Last of all, for the random forest model, the best hyperparameters for the random forest model, based on @tbl-rf-hyper, are 14 for the number of randomly drawn candidate and 2 for the minimum number of models.

For the number of randomly drawn candidate variables in my random forest and boosted tree models, I had my range for tuning set to a maximum of 14, so it is likely that further tuning could be explored by increasing this value when setting tuning limits.

## Feature Engineered Recipe Model Results
After analyzing the best hyperparameters for the appropriate models, I compared the results of the models that used the feature engineered recipe to one another with the metric of RMSE.

```{r}
#| echo: false
#| tbl-cap: "Comparison of RMSEs of Each Model"
#| label: tbl-rmse-comparison

engineered_final_table |> 
  knitr::kable()
```

Based on @tbl-rmse-comparison, the random forest model will act as the final model, as it it has the lowest RMSE value out of the six models, indicating lower error and therefore stronger accuracy. I was not surprised that the random forest model won because of its complexity relative to the other models, although I was surprised at how strong the ordinary linear regression performed, with an RMSE of 0.3661882, because I did not tune any parameters for it. I expected the models that involved tuning hyperparameters, such as the boosted tree, elastic net, and k-nearest neighbors model, to perform better than the ordinary linear regression model, although the ordinary linear regression model may have had a lower RMSE because I removed some categorical data, such as the color and whether or not the vehicle had gas, that I deemed to be disruptive to the model.

## Final Model Selection
(insert the combined table here and say which one is gonna be da final!!!!! and compare how well each does).

# Final Model Analysis

After determining that the best model was the random forest model, I fit and trained the final model using the training dataset and predicted the values of the testing dataset.
```{r}
#| echo: false
#| tbl-cap: "Final Model Statistics"
#| label: tbl-final-model-stats

final_predict_stats |> 
  knitr::kable()
```
Based on @tbl-final-model-stats, the RMSE is 0.3639547, the mean average error (MAE) is 0.2485033, and the $R^2$ is 0.8778823. Although the RMSE is slightly higher than that of the RMSE for the random forest model as seen in @tbl-rmse-comparison, the RMSE is relatively lower than all the other models. Additionally, the MAE is quite low, indicating less error involved in the model. The value for $R^2$ indicates that approximately 87.78% of the results can be explained by the model, which is quite high. Next, I comapred the results of the predicted and actual values on a graph. (rmse penalizes outliers more)

```{r}
#| echo: false
#| fig-cap: "Observed vs. Predicted Log Transformed Prices (USD)"
#| label: fig-log-price-graph

predict_vs_obs_plot
```
Looking at @fig-log-price-graph, the values appear to be highly close together relative to predicted and observed values. However, let us look at the transformed price levels to see the accuracy of the model, because log values can be grouped much closer than the actual price.

```{r}
#| echo: false
#| fig-cap: "Observed vs. Predicted Prices (USD)"
#| label: fig-price-graph

predict_vs_obs_plot_original
```
Based on @fig-price-graph, there appears to be more variation between the actual prices and predicted prices. More specifically, there tends to be some prices that are much higher in reality in comparison to the predicted price, but for the majority of the results, the predicted and actual prices appear quite close to one another. 

According to the results of @fig-log-price-graph and @fig-log-price-graph, I believe that it was justified to use a random forest model due to the strength of its predictive performance. Additionally, based on @tbl-rmse-comparison, the RMSE for the baseline model is much higher than that of the final model. Thus, I think the effort involved in creating the model made sense, as it allowed for more accurate predictions. I believe the random forest model performed the best because of its flexibility in handling nonlinear relationships, because there were many variables that did not immediately have noticeable correlations due to their nonlinearity when graphed and calculated.

# Conclusions
After working on training and creating my predictive models, I learned that the best model for predicting used car prices was the random forest model using a feature engineered recipe, with significant improvements upon the baseline null model. It appears that tree-based models perform well in this dataset due to its ability to deal with nonlinearity and have flexibility with its variables, as the used cars dataset I used had high levels of nonlinearity in its numeric variables. Using a feature engineered recipe allowed for stronger predictive ability in comparison to the baseline null model.

For future models,

mention what did the best, what to do in the future, and also future insights/things to explore (research questions for the future)

## References


## Appendix
I chose to explore the training data to see if there were any relationships between variables that would be important to note when building the recipe for the model that can affect the strength and performance of the model.

```{r}
#| echo: false
#| tbl-cap: Correlation Matrix of Numerical Variables in Training Dataset
#| label: tbl-corr-matrix

cars_train_corr |> 
  knitr::kable()
```
To begin, I created a correlation matrix to see what numerical variables were correlated with one another. Based on @tbl-corr-matrix, there appear to be correlations between `duration_listed` and `up_counter`.

```{r}
#| echo: false
#| label: fig-original-vs-log
#| fig-cap: "Original vs Log Distributions"

log_original_engine
log_original_photos
```

Next, I explored the distributions of the numerical variables in my training dataset. Based on my findings, it appears that the variables `engine_capacity` and `number_of_photos` had highly skewed distributions. Therefore, I believe that a log transformation would be appropriate for these variables. According to @fig-original-vs-log-1 and @fig-original-vs-log-2, the log transformations for these variables help make it more balanced out, allowing for a potentially more effective model.

```{r}
#| echo: false
#| fig-cap: "Exploration of Categorical Variables"
#| label: fig-test

categorical_exploration_1
```
```{r}
#| echo: false
#| fig-cap: "Car Year Produced Compared to Manufacturer"
#| label: fig-yahoo

manf
```

Last of all, I explored how categorical variables affected the distributions of numeric variables in the dataset. Based on my exploration, as seen in @fig-test and @fig-yahoo, there were associations between `engine_type` and `engine_capacity`, `is_exchangeable` and `duration_listed`, and `manufacturer_name` and `year_produced`. 


