---
title: "Final Report on Price Prediction Model for Online Used Car Sales"
subtitle: |
  | Class: STAT 301-2, Winter 2024, Northwestern University
author: "Brian Dinh"
date: today
fig-cap-location: top
format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: true
  warning: false
number-sections: true
from: markdown+emoji 
---

::: {.callout-tip icon=false}

## Github Repository Link

[This is the link to the GitHub Repository.](https://github.com/stat301-2-2024-winter/final-project-2-BrianDinh26.git)

:::
```{r}
#| echo: false
#| label: import

# turn off scientific notation
options(scipen=999)

# load in all figures used for this analysis.
library(tidyverse)
library(here)

load(here("figures/data_exploration.rda"))
load(here("figures/best_hyperparameters.rda"))
load(here("figures/rmse_tables.rda"))
load(here("figures/assess_final.rda"))
load(here("figures/best_hyperparameters_1.rda"))
load(here("figures/best_hyperparameters_2.rda"))
load(here("figures/bt_autoplot.rda"))

```



# Introduction {#sec-introduction}
For this project, I used a dataset^[Kirill Lepchenkov's Used Cars Dataset ([see website](https://www.kaggle.com/datasets/lepchenkov/usedcarscatalog/data))] on online used car sales, which described the characteristics of various car listings on the website in factor, logical, and numerical form.

In this predictive model building project, I was motivated to learn more about what variables affect the price of a car being resold and what models were the best for creating price predictions. This sort of model would be useful in figuring out if a person is overvaluing or undervaluing their car price listing relative to the market, or if a buyer is getting a good deal or not online.

# Data Overview {#sec-data-overview}
For the used cars dataset, there are 30 variables and 38,531 observations. There are 10 factor variables, 7 numerical variables, and 13 logical variables. There are 38,531 observations. I will be using the price of the used car in USD as my response variable, which means I will be predicting the price.

```{r}
#| echo: false
#| label: fig-og-log-price
#| fig-cap: "Comparison of Distribution of Original Price and Log Transformed Price in Whole Dataset"

og_v_log_price
```
Based on @fig-og-log-price, the original distribution of prices are extremely skewed. In comparison, the log transformed version of prices, although skewed, is less disparate, and therefore I will use the log transformed version of prices in my model building.

```{r}
#| echo: false
#| label: fig-missing-check
#| fig-cap: "View of Missingness in Whole Dataset by Variable"

missingness_check
```

According to @fig-missing-check, there are only misingness issues with the engine capacity, with 10 observations out of the 38,531 observations registering as not available. To deal with this missingness, I will impute the missing observations with the k-nearest neighbors method in the recipes for the model, meaning that the nearest datapoints to the missing observation will be used to guess what the missing value is.

Additionally, I used an 80/20 split of my training data to explore possible relationships in the training dataset, meaning that 80% of the selected data from the training dataset will be used to explore relationships and 20% of the selected data will be used to evaluate the model as the testing dataset. Based on my exploratory data analysis, there appear to be correlations between the duration listed for the car and the number of upvotes on the site (which means how many times the car has been promoted), the engine type and the engine capacity, if the car is exchangeable for another car and the duration listed for the car, and the manufacturer name and year produced. Additionally, there appears a need to log transform the engine capacity and the number of photos, meaning that the log of the data will be taken, as these datapoints appear to have overly skewed distributions. For more information on the exploration methods, please refer to @sec-appendix.

# Methods {#sec-methods}
For the used cars dataset, I will implement an 80 / 20 split, meaning approximately 80% of the data will be used for training the model, and 20% of the data will be used to test how successful the model is. 

In the models I am building, I am using the Root Mean Squared Error (RMSE) as my primary metric to evaluate because I am using regression to predict the price of a car online. The RMSE is the average difference between values predicted by a model and the actual values, meaning the lower the better. The model with the lowest RMSE relative to the other models will be selected as the final model.

I will be using a total of six models for this report. First off, my baseline model will be the null model, which will serve as a measuring stick for the performance of more complex models. No main arguments will be provided to the null model.

I will be building two linear models that will be using a recipe for parametric models: A standard linear regression model and an elastic net model. The standard linear regression model does not have any parameters that will be tuned. For the elastic net model, I will tune the penalty, which is how much parameters get shrunk, and mixture of the model, which determines the type of regression the model has.

Additionally, I will be building three models that will be using a tree-based recipe: A random forest model, a boosted tree model, and a k-nearest neighbors model. 

The random forest model will tune the minimum number of datapoints for a split in a node over a range of 1 to 9 and the number of predictors that will be randomly sampled at each split over a range of 2 to 15, while the number of trees, which are the splits of prediction branches, being 500. 

For the boosted tree model, I will tune the minimum number of datapoints for a split in a node over a range of 1 to 9, the number of predictors that will be randomly sampled at each split over a range of 2 to 15, and the rate at which the boosting algorithm adapts from iteration-to-iteration over a range of -5 to -0.2, with the number of trees being 500.

For the k-nearest neighbors model, I will tune the number of neighbors in the model, which are the number of close datapoints used for predicting.

The resampling technique used will be v-fold cross-validation, with 10 folds and 5 repeats, meaning there will be 10 sets of performance metrics and 5 repetitions of the process. This process is meant to improve the model's accuracy and prevent overfitting through its repetitions and averaging out the results.

## Recipes {#sec-recipes}
Recipes are feature engineering steps used to prepare the data to use in a predictive model. For this report, I will be using two recipes, each with a variation for tree based recipes that have a different engine from parametric models, which are more linear.

The first recipe that will be used is a baseline, "kitchen sink" recipe, which is a basic recipe with minimal steps that will allow for the model to run. In the baseline recipe I use, I initially remove the model name, location listing, and engine fuel type, because these variables have too many categories that clutter the predictive model. Then, I will convert all categorical variables to numeric variables in order to allow categorical variables to predict our regression result. Then, to deal with missingness for engine capacity, I will use the k-nearest neighbors method to predict to predict the missing variables using the data closest to it. Then, I will remove all variables with zero variance, because variables with zero variance do not give any information on what can help predict the price if all the values are the same. Last of all, I will normalize all variables, as it will help put the data on a similar scale and therefore improve the performance of the model.

I also created a tree-based verison of this recipe to use for tree-based models, where the difference is that when the categorical variables are converted to numerical variables, they are one hot encoded, which prepares the data better for the model by providing more complex labels to the data.

The second recipe used will be a feature engineered recipe, where the recipe is modified to include possible interactions and transformations that appear to be correlated or related. For the first part of my recipe, I removed the model name, location listing, engine fuel type, color, and if the engine has gas from the dataset due to exploration that saw these variables having too many categories that cluttered the predictive model.  Then, I will convert all categorical variables to numeric variables in order to allow categorical variables to predict our regression result. Then, to deal with missingness for engine capacity, I will use the k-nearest neighbors method to predict to predict the missing variables using the data closest to it. Then, I will convert the engine capacity and number of photos to a log scale to deal with the skewed distribution fo the variables. Then I will create interactions between the duration listed and number of upvotes, the engine type and engine capacity, if the car is exchangeable for another car and the duration listed, and the manufcaturer name and year produced due to data exploration performed in @sec-data-exploration.

Additionally, there will be a feature engineered recipe for tree-based models based on the second, feature engineered recipe. The recipe is almost the same as the second recipe, but all factor variables will instead be one-hot encoded because the recipe will be used on tree-based models, allowing for stronger predictions.

# Model Building & Selection Results
## Kitchen Sink Recipe Model Results
Initially, I applied the baseline, "kitchen sink" recipes to the models model and found the RMSE for each, as RMSE is the metric by which I am comparing models and will determine the final model. 

```{r}
#| echo: false
#| tbl-cap: "RMSEs of Baseline Recipe Trained Models"
#| label: tbl-rmse-baseline 

kitchen_sink_table |> 
  knitr::kable()
```

According to @tbl-rmse-baseline, the best performing model using the kitchen sink recipe was the boosted tree model, with an RMSE of 0.3387086, which is significantly lower than the second-lowest model, the random forest model.

Then, I found the best hyperparameters for the models that were tuned, based on the lowest RMSE found for each model that used the kitchen sink recipes. For the null model and the ordinary linear regression model, there was no tuning of hyperparameters involved because they do not have any relevant hyperparameters to look at.

```{r}
#| echo: false
#| tbl-cap: "Best Hyperparameters for the Elastic Net Model, Kitchen Sink"
#| label: tbl-elastic-hyper

elastic_select_1 |> 
  knitr::kable(digits = c(15, 1, NA))
```
According to @tbl-elastic-hyper, the best hyperparameters for the elastic net model with the kitchen sink recipe is a penalty value of 0.01778279, and a mixture of 0.2, indicating a mix leaning more towards ridge regression than lasso regerssion.

```{r}
#| echo: false
#| tbl-cap: "Best Hyperparameters for the K-Nearest Neighbors Model, Kitchen Sink"
#| label: tbl-knn-hyper

knn_select_1 |> 
  knitr::kable()
```
Based on @tbl-knn-hyper, the best model for the k-nearest neighbors model with the kitchen sink recipe has tuned a value of 15 for neighbors, indicating that the greater the neighbors, the lower the RMSE.

```{r}
#| echo: false
#| tbl-cap: "Best Hyperparameters for the Boosted Tree Model, Kitchen Sink"
#| label: tbl-bt-hyper

bt_select_1 |> 
  knitr::kable()
```
According to @tbl-bt-hyper, the best hyperparameters for the boosted tree model with the kitchen sink recipe are 9 for the number of randomly drawn candidate variables, 5 for the minimum number of variables before a split, and a learn rate of approximately 0.0398107, which

```{r}
#| echo: false
#| tbl-cap: "Best Hyperparameters for the Random Forest Model, Kitchen Sink"
#| label: tbl-rf-hyper

rf_select_1 |> 
  knitr::kable()
```
Last of all, for the random forest model, the best hyperparameters for the random forest model with the kitchen sink recipe, based on @tbl-rf-hyper, are 9 for the number of randomly drawn candidate variables and 2 for the minimum number of variables before a split. It appears that for both the boosted tree and random forest model, the higher the number of randomly drawn candidate variables, the lower the RMSE, while the minimum number of variables before a split varied.


## Feature Engineered Recipe Model Results
Next, I used the feature engineered recipes to the models model and found the RMSE for each.

```{r}
#| echo: false
#| tbl-cap: "RMSEs of Feature Engineered Recipe Trained Models"
#| label: tbl-rmse-fe 

engineered_final_table |> 
  knitr::kable()
```

According to @tbl-rmse-fe, the best performing model using the kitchen sink recipe was the boosted tree model, with an RMSE of 0.3395575. It appears that the ordinary linear regression and random forest models made improvements in comparison to @tbl-rmse-baseline.

Then, I found the best hyperparameters for the models that were tuned, based on the lowest RMSE found for each model that used the feature engineered recipes. Once again, for the null model and the ordinary linear regression model, there was no tuning of hyperparameters involved because they do not have any relevant hyperparameters to look at.

```{r}
#| echo: false
#| tbl-cap: "Best Hyperparameters for the Elastic Net Model, Feature Engineered"
#| label: tbl-elastic-hyper-fe

elastic_select_1 |> 
  knitr::kable(digits = c(15, 1, NA))
```
According to @tbl-elastic-hyper-fe, the best hyperparameters for the elastic net model with the feature engineered recipe is a penalty value of 0.01778279, and a mixture of 0.2, which are the exact same values as that of the best hyperparameters for the kitchen sink elastic net model in @tbl-elastic-hyper. For future reference, one may consider increasing the penalty as a means of increasing model performance.

```{r}
#| echo: false
#| tbl-cap: "Best Hyperparameters for the K-Nearest Neighbors Model, Feature Engineered"
#| label: tbl-knn-hyper-fe

knn_select_1 |> 
  knitr::kable()
```
Based on @tbl-knn-hyper-fe, the best model for the k-nearest neighbors model with the feature engineered recipe has tuned a value of 15 for neighbors, which is the same as the best hyperparameter for the kitchen sink k-nearest neighbors model in @tbl-knn-hyper, indicating that there is a general trend for more neighbors to produce a stronger model. For future models, one may increase the range of neighbors allowed for the model when tuning.

```{r}
#| echo: false
#| tbl-cap: "Best Hyperparameters for the Boosted Tree Model, Feature Engineered"
#| label: tbl-bt-hyper-fe

bt_select_1 |> 
  knitr::kable()
```
According to @tbl-bt-hyper-fe, the best hyperparameters for the boosted tree model with the feature engineered recipe are 9 for the number of randomly drawn candidate variables, 5 for the minimum number of variables before a split, and a learn rate of approximately 0.0398107, which are the same best hyperparameter values for the boosted tree model with the kitchen sink recipe in @tbl-bt-hyper. In future explorations or analysis, the minimum number of variables before a split may be increased in order to increase performance due to the number of variables in this dataset.

```{r}
#| echo: false
#| tbl-cap: "Best Hyperparameters for the Random Forest Model, Feature Engineered"
#| label: tbl-rf-hyper-fe

rf_select_1 |> 
  knitr::kable()
```
Last of all, for the random forest model, the best hyperparameters for the random forest model with the kitchen sink recipe, based on @tbl-rf-hyper-fe, are 9 for the number of randomly drawn candidate variables and 2 for the minimum number of variables before a split, which are the same as in @tbl-rf-hyper.

## Final Model Selection
After analyzing the best hyperparameters for the appropriate models, I compared the results of the models that used the feature engineered recipe to one another with the metric of RMSE.

```{r}
#| echo: false
#| tbl-cap: "Comparison of RMSEs of Models using Both Recipes"
#| label: tbl-rmse-comparison

comparison_table |> 
  knitr::kable()
```
Based on @tbl-rmse-comparison, it turns out that the boosted tree model using the kitchen sink recipe performed the best out of all the models, with an RMSE of 0.3387086 that slightly beat out the boosted tree model using the feature engineered recipe. I was not surprised that the boosted tree model won because of its complexity compared to other models and its ability to deal with nonlinear relationships well, but I was surprised that the kitchen sink recipe was the best because I thought the feature engineered aspects would have improved the model. I do think that the slight boost in performance from the kitchen sink recipe was due to the removal of two categorical variables in the feature engineered recipe in comparison to the kitchen sink recipe, meaning those variables might have had more value in creating accurate predictions than I initially thought. There are also interesting improvements for the ordinary linear regression model when comparing the feature engineered recipe and kitchen sink recipe, which may indicate that removing more categorical variables in the dataset can help the strength of the linear model due to removing aspects of nonlinearity. 

```{r}
#| echo: false
#| fig-cap: "Comparison of Hyperparameters for Best Model"
#| label: fig-rmse-best-hyp

bt_autoplot
```
In @fig-rmse-best-hyp, it appears that the best RMSEs occur with a higher number of randomly selected predictors, a greater minimal node size, and a greater learning rate.

In conclusion, the model used for final model analysis will be the kitchen sink version of the boosted tree model.

# Final Model Analysis

After determining that the best model was the random forest model, I fit and trained the final model using the training dataset and predicted the values of the testing dataset.
```{r}
#| echo: false
#| tbl-cap: "Final Model Statistics"
#| label: tbl-final-model-stats

final_predict_stats |> 
  knitr::kable()
```
Based on @tbl-final-model-stats, the RMSE is 0.3403434, the mean average error (MAE) is 0.2315781, and the $R^2$ is 0.8904937 Although the RMSE is slightly higher than that of the RMSE for the random forest model as seen in @tbl-rmse-comparison, the RMSE is relatively lower than all the other models. Additionally, the MAE is quite low, indicating less error involved in the model. The value for $R^2$ indicates that approximately 87.78% of the results can be explained by the model, which is quite high. Next, I compared the results of the predicted and actual values on a graph.

```{r}
#| echo: false
#| fig-cap: "Observed vs. Predicted Log Transformed Prices (USD)"
#| label: fig-log-price-graph

predict_vs_obs_plot
```
Looking at @fig-log-price-graph, the values appear to be highly close together relative to predicted and observed values. However, let us look at the transformed price levels to see the accuracy of the model, because log values can be grouped much closer than the actual price.

```{r}
#| echo: false
#| fig-cap: "Observed vs. Predicted Prices (USD)"
#| label: fig-price-graph

predict_vs_obs_plot_original
```
Based on @fig-price-graph, there appears to be more variation between the actual prices and predicted prices. More specifically, there tends to be some prices that are much higher in reality in comparison to the predicted price, but for the majority of the results, the predicted and actual prices appear quite close to one another. 

According to the results of @fig-log-price-graph and @fig-price-graph, I believe that it was justified to use a boosted tree model due to the strength of its predictive performance. Additionally, based on @tbl-rmse-comparison, the RMSE for the null model is much higher than that of the final model selected. Thus, I think the effort involved in creating the model made sense, as it allowed for more accurate predictions. I believe the boosted tree model performed the best because of its flexibility in handling nonlinear relationships, because there were many variables that did not immediately have noticeable correlations due to their nonlinearity when graphed and calculated. However, for future explorations, I believe having a recipe that is more deeply explored may allow for a stronger performance, because currently, the kitchen sink recipe with the boosted tree model performs the best.

# Conclusions
After working on training and creating my predictive models, I learned that the best model for predicting used car prices was the boosted tree model using a kitchen sink recipe, with significant improvements upon the baseline null model. It appears that tree-based models perform well in this dataset due to its ability to deal with nonlinearity and have flexibility with its variables, as the used cars dataset I used had high levels of nonlinearity in its numeric variables. Although the feature engineered recipe allowed for stronger predictive ability for the ordinary linear regression and random forest model, the boosted tree model with the feature engineered recipe was actually slightly worse than the kitchen sink boosted tree model. It appears that for boosted tree models, having the greatest number of selected predictors only slightly increased performance, while greater learning rates and minimal node size greatly improved model performance.

For future models, it may be beneficial to increase the minimal node size and learning rate ranges in order to increase performance. Additionally, for future recipes, possibly refrain from removing more categorical variables dude to the strength of tree based models when provided more categorical variable information. Possible research questions for the future would be to analyze what used cars are actually sold on such used cars websites and predict whether or not a car has been sold or not. Additionally, one may explore how used car prices compare to the current car market as well for newer models and see if the general prices of new cars affects how used cars are priced.

# References
- Lepchenkov, Kirill (2019, December). *Used-cars-catalog*. Kaggle. [https://www.kaggle.com/datasets/lepchenkov/usedcarscatalog/data](https://www.kaggle.com/datasets/lepchenkov/usedcarscatalog/data)


# Appendix {#sec-appendix}

## Exploratory Data Analysis {#sec-data-exploration}
I chose to explore the training data to see if there were any relationships between variables that would be important to note when building the recipe for the model that can affect the strength and performance of the model.

```{r}
#| echo: false
#| tbl-cap: Correlation Matrix of Numerical Variables in Training Dataset
#| label: tbl-corr-matrix

cars_train_corr |> 
  knitr::kable()
```
To begin, I created a correlation matrix to see what numerical variables were correlated with one another. Based on @tbl-corr-matrix, there appear to be correlations between the duration listed of the car and the number of upvotes the car has on the site.

```{r}
#| echo: false
#| label: fig-original-vs-log
#| fig-cap: "Original vs Log Distributions"

log_original_engine
log_original_photos
```

Next, I explored the distributions of the numerical variables in my training dataset. Based on my findings, it appears that the variables engine capacity and the number of photos had highly skewed distributions. Therefore, I believe that a log transformation would be appropriate for these variables. According to @fig-original-vs-log-1 and @fig-original-vs-log-2, the log transformations for these variables help make it more balanced out, allowing for a potentially more effective model.

```{r}
#| echo: false
#| fig-cap: "Exploration of Categorical Variables"
#| label: fig-cat-exp

categorical_exploration_1
```
```{r}
#| echo: false
#| fig-cap: "Car Year Produced Compared to Manufacturer"
#| label: fig-car-prod

manf
```

Based on my exploration, as seen in @fig-cat-exp and @fig-car-prod, there were associations between the engine type and the engine capacity, if the car is exchangeable for another car and the duration listed for the car, and the manufacturer name and year produced.

